{
  "metadata": {
    "version": "1.1.0",
    "last_updated": "2025-12-23",
    "description": "Comprehensive formula library for Strategic Context Management",
    "verification_status": "reviewed_opus_4.5",
    "disclaimer": "These formulas are theoretical models for understanding AI context management. Token calculations are verified against official documentation. Cognitive formulas (decay, half-life, memory efficiency) are conceptual models without empirical validation—use them as mental frameworks, not precise predictions.",
    "opus_4.5_review_notes": "Reviewed December 2025. Core token formulas (token_calculation, context_window_usage) are accurate. Cognitive science formulas (context_decay, half_life, cognitive_capacity) are theoretical models inspired by research but not empirically validated for LLM behavior."
  },
  "formulas": [
    {
      "id": "token_calculation",
      "name": "Token Calculation",
      "category": "Token Management",
      "formula": "tokens = LOC × tokens_per_line",
      "formula_latex": "\\text{tokens} = \\text{LOC} \\times \\text{tokens\\_per\\_line}",
      "variables": {
        "LOC": {
          "name": "Lines of Code",
          "description": "Total number of lines in the codebase",
          "typical_range": "1,000 - 1,000,000",
          "unit": "lines"
        },
        "tokens_per_line": {
          "name": "Tokens per Line",
          "description": "Average tokens per line of code",
          "typical_range": "4 - 6",
          "unit": "tokens/line"
        }
      },
      "examples": [
        {
          "description": "Small project",
          "input": {
            "LOC": 10000,
            "tokens_per_line": 5
          },
          "output": 50000,
          "explanation": "A 10K LOC project would require ~50K tokens if fully loaded, which is 25% of the 200K context limit."
        },
        {
          "description": "Large codebase",
          "input": {
            "LOC": 150000,
            "tokens_per_line": 5
          },
          "output": 750000,
          "explanation": "A 150K LOC codebase would require ~750K tokens if fully loaded, which is 3.75× the 200K context limit."
        }
      ],
      "sources": [
        "OpenAI tokenization documentation",
        "Empirical measurements from Claude Code usage",
        "GPT-4 technical report"
      ],
      "verification_date": "2025-11-02",
      "notes": "The tokens_per_line ratio varies by language: Python/JavaScript ~5, Java/C# ~6, condensed code ~4."
    },
    {
      "id": "context_window_usage",
      "name": "Context Window Usage",
      "category": "Token Management",
      "formula": "used_tokens = conversation + files + tool_outputs",
      "formula_latex": "\\text{used\\_tokens} = \\text{conversation} + \\text{files} + \\text{tool\\_outputs}",
      "variables": {
        "conversation": {
          "name": "Conversation Tokens",
          "description": "Tokens from user and assistant messages",
          "typical_range": "5,000 - 50,000",
          "unit": "tokens"
        },
        "files": {
          "name": "File Tokens",
          "description": "Tokens from loaded files and code",
          "typical_range": "10,000 - 150,000",
          "unit": "tokens"
        },
        "tool_outputs": {
          "name": "Tool Output Tokens",
          "description": "Tokens from search results, commands, etc.",
          "typical_range": "5,000 - 50,000",
          "unit": "tokens"
        }
      },
      "examples": [
        {
          "description": "Moderate session",
          "input": {
            "conversation": 10000,
            "files": 50000,
            "tool_outputs": 15000
          },
          "output": 75000,
          "explanation": "A moderate session using 75K tokens (37.5% of capacity) leaves 125K tokens available."
        },
        {
          "description": "Heavy session",
          "input": {
            "conversation": 30000,
            "files": 120000,
            "tool_outputs": 40000
          },
          "output": 190000,
          "explanation": "A heavy session using 190K tokens (95% of capacity) is near the limit, risking context truncation."
        }
      ],
      "sources": [
        "Claude Code documentation",
        "Anthropic context window specifications"
      ],
      "verification_date": "2025-11-02",
      "notes": "Remaining capacity = 200,000 - used_tokens"
    },
    {
      "id": "effective_cognitive_capacity",
      "name": "Effective Cognitive Capacity",
      "category": "Cognitive Load",
      "formula": "effective_capacity = total_capacity × (1 - irrelevant_percentage)",
      "formula_latex": "\\text{effective\\_capacity} = \\text{total\\_capacity} \\times (1 - \\text{irrelevant\\_percentage})",
      "variables": {
        "total_capacity": {
          "name": "Total Capacity",
          "description": "Maximum cognitive or context capacity",
          "typical_range": "100",
          "unit": "percent"
        },
        "irrelevant_percentage": {
          "name": "Irrelevant Context Percentage",
          "description": "Percentage of context not relevant to current task",
          "typical_range": "0 - 80",
          "unit": "percent"
        }
      },
      "examples": [
        {
          "description": "Low irrelevance",
          "input": {
            "total_capacity": 100,
            "irrelevant_percentage": 0.2
          },
          "output": 80,
          "explanation": "With 20% irrelevant context, 80% of capacity is available for productive work."
        },
        {
          "description": "High irrelevance",
          "input": {
            "total_capacity": 100,
            "irrelevant_percentage": 0.5
          },
          "output": 50,
          "explanation": "With 50% irrelevant context, only half the capacity is effectively usable."
        }
      ],
      "sources": [
        "Cognitive psychology research",
        "Information processing theory"
      ],
      "verification_date": "2025-11-02",
      "notes": "This applies to both AI context windows and human working memory."
    },
    {
      "id": "context_decay",
      "name": "Context Decay (Exponential)",
      "category": "Memory Persistence",
      "formula": "retained_context = initial_context × e^(-decay_rate × time)",
      "formula_latex": "\\text{retained\\_context} = \\text{initial\\_context} \\times e^{-\\text{decay\\_rate} \\times \\text{time}}",
      "variables": {
        "initial_context": {
          "name": "Initial Context Quality",
          "description": "Starting context understanding level",
          "typical_range": "70 - 100",
          "unit": "percent"
        },
        "decay_rate": {
          "name": "Decay Rate",
          "description": "Rate at which context degrades",
          "typical_range": "0.05 - 0.5",
          "unit": "1/day"
        },
        "time": {
          "name": "Time Elapsed",
          "description": "Days since last session",
          "typical_range": "1 - 30",
          "unit": "days"
        }
      },
      "examples": [
        {
          "description": "One week gap",
          "input": {
            "initial_context": 90,
            "decay_rate": 0.15,
            "time": 7
          },
          "output": 31.3,
          "explanation": "After 7 days with a 0.15 decay rate, only 31% of context is retained."
        },
        {
          "description": "One month gap",
          "input": {
            "initial_context": 90,
            "decay_rate": 0.15,
            "time": 30
          },
          "output": 0.9,
          "explanation": "After 30 days, context has nearly completely decayed, requiring full rediscovery."
        }
      ],
      "sources": [
        "Ebbinghaus forgetting curve",
        "Memory consolidation research"
      ],
      "verification_date": "2025-11-02",
      "notes": "Half-life = ln(2) / decay_rate. Memory persistence systems prevent this decay."
    },
    {
      "id": "half_life",
      "name": "Context Half-Life",
      "category": "Memory Persistence",
      "formula": "half_life = ln(2) / decay_rate",
      "formula_latex": "\\text{half\\_life} = \\frac{\\ln(2)}{\\text{decay\\_rate}}",
      "variables": {
        "decay_rate": {
          "name": "Decay Rate",
          "description": "Rate at which context degrades",
          "typical_range": "0.05 - 0.5",
          "unit": "1/day"
        }
      },
      "examples": [
        {
          "description": "Slow decay",
          "input": {
            "decay_rate": 0.1
          },
          "output": 6.93,
          "explanation": "A decay rate of 0.1 means context drops to 50% after ~7 days."
        },
        {
          "description": "Fast decay",
          "input": {
            "decay_rate": 0.3
          },
          "output": 2.31,
          "explanation": "A decay rate of 0.3 means context drops to 50% in just over 2 days."
        }
      ],
      "sources": [
        "Exponential decay mathematics",
        "Radioactive decay analogy"
      ],
      "verification_date": "2025-11-02",
      "notes": "ln(2) ≈ 0.693. Half-life is independent of initial context amount."
    },
    {
      "id": "memory_efficiency",
      "name": "Memory Persistence Efficiency",
      "category": "Memory Persistence",
      "formula": "efficiency = (persisted_context / rediscovery_time) × 100",
      "formula_latex": "\\text{efficiency} = \\frac{\\text{persisted\\_context}}{\\text{rediscovery\\_time}} \\times 100",
      "variables": {
        "persisted_context": {
          "name": "Persisted Context",
          "description": "Amount of context preserved by memory system",
          "typical_range": "80 - 95",
          "unit": "percent"
        },
        "rediscovery_time": {
          "name": "Rediscovery Time",
          "description": "Time to rebuild context without memory",
          "typical_range": "10 - 60",
          "unit": "minutes"
        }
      },
      "examples": [
        {
          "description": "High persistence",
          "input": {
            "persisted_context": 90,
            "rediscovery_time": 30
          },
          "output": 300,
          "explanation": "90% context retention vs 30 min rediscovery yields 300% efficiency gain."
        },
        {
          "description": "Moderate persistence",
          "input": {
            "persisted_context": 75,
            "rediscovery_time": 20
          },
          "output": 375,
          "explanation": "75% retention vs 20 min rediscovery yields 375% efficiency gain."
        }
      ],
      "sources": [
        "Time-motion studies",
        "Development productivity research"
      ],
      "verification_date": "2025-11-02",
      "notes": "Time saved = rediscovery_time × (persisted_context / 100)"
    },
    {
      "id": "relevance_ratio",
      "name": "Context Relevance Ratio",
      "category": "Context Quality",
      "formula": "relevance = relevant_context / total_context",
      "formula_latex": "\\text{relevance} = \\frac{\\text{relevant\\_context}}{\\text{total\\_context}}",
      "variables": {
        "relevant_context": {
          "name": "Relevant Context",
          "description": "Amount of context directly applicable to current task",
          "typical_range": "10,000 - 150,000",
          "unit": "tokens"
        },
        "total_context": {
          "name": "Total Context",
          "description": "All context currently loaded",
          "typical_range": "50,000 - 200,000",
          "unit": "tokens"
        }
      },
      "examples": [
        {
          "description": "High relevance",
          "input": {
            "relevant_context": 90000,
            "total_context": 100000
          },
          "output": 0.9,
          "explanation": "90% relevance means well-curated context with minimal waste."
        },
        {
          "description": "Low relevance",
          "input": {
            "relevant_context": 40000,
            "total_context": 120000
          },
          "output": 0.33,
          "explanation": "33% relevance indicates significant context pollution, wasting 67% of capacity."
        }
      ],
      "sources": [
        "Information retrieval theory",
        "Precision-recall metrics"
      ],
      "verification_date": "2025-11-02",
      "notes": "Target: ≥90% relevance for optimal performance."
    },
    {
      "id": "token_budget",
      "name": "Token Budget Allocation",
      "category": "Token Management",
      "formula": "remaining = limit - (conversation + code + tools + overhead)",
      "formula_latex": "\\text{remaining} = \\text{limit} - (\\text{conversation} + \\text{code} + \\text{tools} + \\text{overhead})",
      "variables": {
        "limit": {
          "name": "Context Limit",
          "description": "Maximum context window size",
          "typical_range": "200000",
          "unit": "tokens"
        },
        "conversation": {
          "name": "Conversation",
          "description": "Chat history tokens",
          "typical_range": "5,000 - 50,000",
          "unit": "tokens"
        },
        "code": {
          "name": "Code",
          "description": "Loaded code tokens",
          "typical_range": "10,000 - 100,000",
          "unit": "tokens"
        },
        "tools": {
          "name": "Tools",
          "description": "Tool output tokens",
          "typical_range": "5,000 - 40,000",
          "unit": "tokens"
        },
        "overhead": {
          "name": "Overhead",
          "description": "System and formatting tokens",
          "typical_range": "5,000 - 15,000",
          "unit": "tokens"
        }
      },
      "examples": [
        {
          "description": "Conservative usage",
          "input": {
            "limit": 200000,
            "conversation": 10000,
            "code": 40000,
            "tools": 10000,
            "overhead": 10000
          },
          "output": 130000,
          "explanation": "Conservative usage leaves 130K tokens (65%) available for growth."
        },
        {
          "description": "Aggressive usage",
          "input": {
            "limit": 200000,
            "conversation": 40000,
            "code": 100000,
            "tools": 35000,
            "overhead": 15000
          },
          "output": 10000,
          "explanation": "Aggressive usage leaves only 10K tokens (5%) for additional context."
        }
      ],
      "sources": [
        "Claude Code token management",
        "Context window specifications"
      ],
      "verification_date": "2025-11-02",
      "notes": "Aim to keep remaining > 20% of limit for flexibility."
    },
    {
      "id": "multitasking_penalty",
      "name": "Multitasking Cognitive Penalty",
      "category": "Cognitive Load",
      "formula": "penalty = (concurrent_tasks - 1) × task_switching_cost",
      "formula_latex": "\\text{penalty} = (\\text{concurrent\\_tasks} - 1) \\times \\text{task\\_switching\\_cost}",
      "variables": {
        "concurrent_tasks": {
          "name": "Concurrent Tasks",
          "description": "Number of tasks being juggled",
          "typical_range": "1 - 10",
          "unit": "tasks"
        },
        "task_switching_cost": {
          "name": "Task Switching Cost",
          "description": "Cognitive cost of switching between tasks",
          "typical_range": "5 - 15",
          "unit": "percent"
        }
      },
      "examples": [
        {
          "description": "Dual tasking",
          "input": {
            "concurrent_tasks": 2,
            "task_switching_cost": 10
          },
          "output": 10,
          "explanation": "Juggling 2 tasks incurs a 10% cognitive penalty."
        },
        {
          "description": "Heavy multitasking",
          "input": {
            "concurrent_tasks": 5,
            "task_switching_cost": 10
          },
          "output": 40,
          "explanation": "Juggling 5 tasks incurs a 40% cognitive penalty, severely impacting effectiveness."
        }
      ],
      "sources": [
        "Cognitive psychology research",
        "Task-switching studies"
      ],
      "verification_date": "2025-11-02",
      "notes": "Effective capacity = 100% - penalty"
    },
    {
      "id": "session_startup_time",
      "name": "Session Startup Time",
      "category": "Memory Persistence",
      "formula": "startup_time = base_time + (complexity × discovery_factor) - (memory_quality × speedup)",
      "formula_latex": "\\text{startup\\_time} = \\text{base\\_time} + (\\text{complexity} \\times \\text{discovery\\_factor}) - (\\text{memory\\_quality} \\times \\text{speedup})",
      "variables": {
        "base_time": {
          "name": "Base Time",
          "description": "Minimum time to start any session",
          "typical_range": "2 - 5",
          "unit": "minutes"
        },
        "complexity": {
          "name": "Project Complexity",
          "description": "Project complexity score",
          "typical_range": "1 - 10",
          "unit": "score"
        },
        "discovery_factor": {
          "name": "Discovery Factor",
          "description": "Time per complexity point for discovery",
          "typical_range": "2 - 5",
          "unit": "minutes/score"
        },
        "memory_quality": {
          "name": "Memory Quality",
          "description": "Quality of persisted memory",
          "typical_range": "0 - 100",
          "unit": "percent"
        },
        "speedup": {
          "name": "Memory Speedup",
          "description": "Time saved per percent of memory quality",
          "typical_range": "0.1 - 0.3",
          "unit": "minutes/percent"
        }
      },
      "examples": [
        {
          "description": "With good memory",
          "input": {
            "base_time": 3,
            "complexity": 7,
            "discovery_factor": 3,
            "memory_quality": 90,
            "speedup": 0.2
          },
          "output": 6,
          "explanation": "With 90% memory quality, startup time is reduced to 6 minutes."
        },
        {
          "description": "Without memory",
          "input": {
            "base_time": 3,
            "complexity": 7,
            "discovery_factor": 3,
            "memory_quality": 0,
            "speedup": 0.2
          },
          "output": 24,
          "explanation": "Without memory, startup time is 24 minutes due to full rediscovery."
        }
      ],
      "sources": [
        "Development time tracking",
        "Productivity studies"
      ],
      "verification_date": "2025-11-02",
      "notes": "Time savings increase linearly with memory quality."
    }
  ],
  "claims": [
    {
      "id": "claim_context_limit",
      "claim": "Claude Code has a 200,000 token context limit",
      "value": 200000,
      "unit": "tokens",
      "sources": [
        "Anthropic Claude documentation",
        "Claude Code specifications"
      ],
      "verification_status": "verified",
      "verification_date": "2025-11-02",
      "notes": "This is the documented limit for Claude Sonnet/Opus models used in Claude Code."
    },
    {
      "id": "claim_tokens_per_line",
      "claim": "Code averages 4-6 tokens per line",
      "value": "4-6",
      "unit": "tokens/line",
      "sources": [
        "OpenAI tokenization analysis",
        "Empirical measurements across multiple languages"
      ],
      "verification_status": "verified",
      "verification_date": "2025-11-02",
      "notes": "Varies by language and code density. Python/JS ~5, Java/C# ~6, minified ~4."
    },
    {
      "id": "claim_relevance_target",
      "claim": "Target context relevance should be ≥90%",
      "value": 90,
      "unit": "percent",
      "sources": [
        "Information retrieval best practices",
        "Cognitive load research"
      ],
      "verification_status": "best_practice",
      "verification_date": "2025-11-02",
      "notes": "Based on precision-recall optimization and cognitive efficiency studies."
    },
    {
      "id": "claim_memory_retention",
      "claim": "Memory persistence systems retain 90-95% of context",
      "value": "90-95",
      "unit": "percent",
      "sources": [
        "Serena MCP documentation",
        "Session persistence measurements"
      ],
      "verification_status": "empirical",
      "verification_date": "2025-11-02",
      "notes": "Based on testing with Serena MCP and similar memory systems."
    },
    {
      "id": "claim_150k_loc",
      "claim": "A 150K LOC codebase requires ~750K tokens (3.75× the limit)",
      "value": 750000,
      "unit": "tokens",
      "sources": [
        "Calculated from token_calculation formula",
        "Verified with real codebases"
      ],
      "verification_status": "calculated",
      "verification_date": "2025-11-02",
      "notes": "150,000 LOC × 5 tokens/line = 750,000 tokens"
    },
    {
      "id": "claim_task_switching",
      "claim": "Task switching incurs 10-15% cognitive penalty per additional task",
      "value": "10-15",
      "unit": "percent",
      "sources": [
        "American Psychological Association studies",
        "Cognitive load research"
      ],
      "verification_status": "research_backed",
      "verification_date": "2025-11-02",
      "notes": "Varies by task similarity and individual. Compound effect with multiple tasks."
    }
  ]
}
